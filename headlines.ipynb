{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = (6,6)\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer\n",
    "from keras import initializers, regularizers, constraints\n",
    "\n",
    "from keras.layers import Dense, Input, LSTM, Bidirectional, Activation, Conv1D, GRU, TimeDistributed\n",
    "from keras.layers import Dropout, Embedding, GlobalMaxPooling1D, MaxPooling1D, Add, Flatten, SpatialDropout1D\n",
    "from keras.layers import GlobalAveragePooling1D, BatchNormalization, concatenate\n",
    "from keras.layers import Reshape, merge, Concatenate, Lambda, Average\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.initializers import Constant\n",
    "from keras.layers.merge import add\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from keras.utils import np_utils\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json('News_Category_Dataset.json', lines=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "82242066898b9d0625cdc65e8773efc0c732c031",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cates = df.groupby('category')\n",
    "print(\"total categories:\", cates.ngroups)\n",
    "print(cates.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "dc7a2544f8bf3fc9f62dfb8d8b41858b77b14700",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "df.category = df.category.map(lambda x: \"WORLDPOST\" if x == \"THE WORLDPOST\" else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a5737eceacbb85f26e2cb640332a60881818b100",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# using headline and short_description as input X\n",
    "\n",
    "df['text'] = df.headline + \" \" + df.short_description\n",
    "\n",
    "# tokenizing\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(df.text)\n",
    "X = tokenizer.texts_to_sequences(df.text)\n",
    "df['words'] = X\n",
    "\n",
    "# delete some empty and short data\n",
    "\n",
    "df['word_length'] = df.words.apply(lambda i: len(i))\n",
    "df = df[df.word_length >= 5]\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "51f586fa92aff7a859d606622788b11fc51fb197",
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.word_length.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ea5417ae1057812e360c413f83a44f029c57df8e",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# using 50 for padding length\n",
    "\n",
    "maxlen = 50\n",
    "X = list(sequence.pad_sequences(df.words, maxlen=maxlen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a788a6ea4d761543ed05faf768aa68b9cf5716dc",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# category to id\n",
    "\n",
    "categories = df.groupby('category').size().index.tolist()\n",
    "category_int = {}\n",
    "int_category = {}\n",
    "for i, k in enumerate(categories):\n",
    "    category_int.update({k:i})\n",
    "    int_category.update({i:k})\n",
    "\n",
    "df['c2id'] = df['category'].apply(lambda x: category_int[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "af4c3b7b877179ba628315d4dc1fe72bf9db8b1d",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_index = tokenizer.word_index\n",
    "\n",
    "EMBEDDING_DIM = 100\n",
    "\n",
    "embeddings_index = {}\n",
    "f = open('../input/glove-global-vectors-for-word-representation/glove.6B.100d.txt')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "print('Total %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8f23e667f067ac480412511082f758f12bf4caf7",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "embedding_layer = Embedding(len(word_index)+1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            embeddings_initializer=Constant(embedding_matrix),\n",
    "                            input_length=maxlen,\n",
    "                            trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "714f2d758c18a70dee1ba7c2d2ac4122da12c45b",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# prepared data \n",
    "\n",
    "X = np.array(X)\n",
    "Y = np_utils.to_categorical(list(df.c2id))\n",
    "\n",
    "# and split to training set and validation set\n",
    "\n",
    "seed = 29\n",
    "x_train, x_val, y_train, y_val = train_test_split(X, Y, test_size=0.2, random_state=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "3a659ff7671aad4e6215681f57725996e7adfccd"
   },
   "source": [
    "# TextCNN algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "502ffd9dd86e982ea58cc0b7cac6b898573a83e1",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inp = Input(shape=(maxlen,), dtype='int32')\n",
    "embedding = embedding_layer(inp)\n",
    "stacks = []\n",
    "for kernel_size in [2, 3, 4]:\n",
    "    conv = Conv1D(64, kernel_size, padding='same', activation='relu', strides=1)(embedding)\n",
    "    pool = MaxPooling1D(pool_size=3)(conv)\n",
    "    drop = Dropout(0.5)(pool)\n",
    "    stacks.append(drop)\n",
    "\n",
    "merged = Concatenate()(stacks)\n",
    "flatten = Flatten()(merged)\n",
    "drop = Dropout(0.5)(flatten)\n",
    "outp = Dense(len(int_category), activation='softmax')(drop)\n",
    "\n",
    "TextCNN = Model(inputs=inp, outputs=outp)\n",
    "TextCNN.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "TextCNN.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "59992f62dd80ccd1805b673f218e91e70a422e0c",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "textcnn_history = TextCNN.fit(x_train, \n",
    "                              y_train, \n",
    "                              batch_size=128, \n",
    "                              epochs=20, \n",
    "                              validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c7bfc92a99c0b1c49a1b10b23eed18f11f77b3af",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "acc = textcnn_history.history['acc']\n",
    "val_acc = textcnn_history.history['val_acc']\n",
    "loss = textcnn_history.history['loss']\n",
    "val_loss = textcnn_history.history['val_loss']\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.plot(epochs, acc, 'red', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'blue', label='Validation acc')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "plt.title('Training and validation loss')\n",
    "plt.plot(epochs, loss, 'red', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'blue', label='Validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_accuracy(model):\n",
    "    predicted = model.predict(x_val)\n",
    "    diff = y_val.argmax(axis=-1) - predicted.argmax(axis=-1)\n",
    "    corrects = np.where(diff == 0)[0].shape[0]\n",
    "    total = y_val.shape[0]\n",
    "    return float(corrects/total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"model TextCNN accuracy: %.6f\" % evaluate_accuracy(TextCNN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
